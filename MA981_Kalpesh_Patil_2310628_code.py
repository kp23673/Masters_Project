# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1stkFKLhZCAz_PnfVmkYIvXJ4yUMmFraj

# Dissertation Code
## Name: Kalpesh Vijay Patil
## ID: 2310628
"""

!pip install datasets
!pip install regex
!pip install yellowbrick

"""# Importing Libraries:"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

"""# Loading  Dataset:"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv(r"/content/drive/MyDrive/Text_data.csv", encoding='latin-1')

df

df.head()

"""- Here is my top 5 rows from my dataset.
- "0" column consists of cancer types and "a" consists of text description.
- There are unnecessary columns and words which is removed in further process.
"""

df.shape

"""There are total 7570 rows and 3 columns.

# Data pre-processing:

## Checking missing values
"""

df.isnull().sum()

"""There are no missing values we can move with forward steps.

## Checking Duplicates values
"""

# Total number of duplicate rows
df.duplicated().sum()

"""There are no duplicate values in this dataset.

## Removing column
"""

df

df = df.drop('Unnamed: 0',axis=1)  # removed unnecessary column
df

df.columns=['Cancer_Type', 'Text_Description']  # changing column names
df

"""- In this section, I removed 'Unnamed: 0' column.
- Also, changed the column names: '0' to 'Cancer_Type' and 'a' to 'Text_Description' for better understanding.

## Checking attributes:
"""

df.info()

"""- The DataFrame occupies approximately 177.5 KB of memory, indicating a relatively lightweight dataset.
- This makes it easy to process and analyze even on systems with limited resources.
- Given this initial overview, further inspection of the columns' content is necessary to determine their roles in the dataset.
- Additionally, the column named Unnamed: 0 should be evaluated to decide whether it is needed for analysis or can be dropped if it's simply a redundant index.







"""

df.describe()

"""The dataset is heavily focused on Thyroid Cancer, which dominates the entries, and there is a variety of descriptions, though one specific description is notably recurrent.

## Removing Outliers
"""

text_sample = df['Text_Description'][2]
text_sample

re.findall(r'[^\x00-\x7F]+', text_sample)[:5]

text_sample2 = re.sub(r'[^\x00-\x7F]+', '', text_sample)
text_sample2

re.findall(r'[^\x00-\x7F]+', text_sample2)[:5]

"""- In this method, I removed unnecessary words and symbols which are
outliers in the 'Text_Description' of this dataset using regex.
- The process is designed to clean text data by identifying and removing any non-ASCII characters from the selected text sample.

## Remove Numbers
"""

df['Text_Description'][0]

re.findall(r'\d+', df['Text_Description'][0])[:10]

df['Text_Description'] = df['Text_Description'].apply(lambda x: re.sub(r'\d+', '', x))

df['Text_Description'][0]

"""In this method, I removed unnecessary numbers from the 'Text_Description' of this dataset using regex.

## Removing Blank Space
"""

df['Text_Description'][0]

re.sub(r'\s+', ' ', df['Text_Description'][0]).strip()

def remove_spaces(data):
  clean_text = re.sub(r'\s+', ' ', data).strip()
  return clean_text

df['Text_Description'] = df['Text_Description'].apply(remove_spaces)

df['Text_Description'][0]

"""In this method, I removed blank spaces from the 'Text_Description' of this dataset using regex for clear understanding.

## Removing Puncuation and Special Characters
"""

df['Text_Description'][3]

re.findall(r'[^a-zA-Z\s]', df['Text_Description'][3])[:10]

len(re.findall(r'[^a-zA-Z\s]', df['Text_Description'][3]))

re.sub(r'[^a-zA-Z\s]', '', df['Text_Description'][3])

df['Text_Description'] = df['Text_Description'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', x))

df['Text_Description'][3]

"""- The code is focused on cleaning up text data by removing any non-alphabetic characters (e.g., digits, punctuation, etc.) from the Text_Description column in a DataFrame.
- The operations ensure that only letters and spaces remain in the text, which might be useful for subsequent text processing tasks where special characters are irrelevant or disruptive.

## Convert text to lower case
"""

df['Text_Description'] = df['Text_Description'].str.lower()

df['Text_Description'][0]

"""This code ensures that the text descriptions are standardized to lowercase, which is common in text preprocessing to avoid case sensitivity issues during analysis.

## Removing stopwords
"""

# removing stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(data):
  clean_text = ' '.join([word for word in data.split() if word not in stop_words])
  return clean_text

  df['Text_Description'] = df['Text_Description'].apply(remove_stopwords)

"""- The code is aimed at preprocessing text data by removing common stopwords from the 'Text_Description' column in a DataFrame.
- Stopwords are often removed in natural language processing (NLP) tasks because they don't contribute much to the meaning or analysis, thus helping in reducing noise and improving the quality of the text for tasks like sentiment analysis, text classification, or topic modeling.
- After applying this, the 'Text_Description' column will contain cleaned text with stopwords removed, potentially making the data more relevant and focused for further analysis.

## Lemmanization
"""

# Lemmantization
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

def lemmatization(data):
  clean_text = ' '.join([lemmatizer.lemmatize(word) for word in data.split()])
  return clean_text

df['Text_Description'] = df['Text_Description'].apply(lemmatization)

df['Text_Description'][0]

"""- The code focuses on lemmatization, a text preprocessing technique where words are reduced to their base or dictionary form (lemma).
-  Applying this to the Text_Description column standardizes the text, improving the quality of data for tasks like sentiment analysis, classification, or any task where understanding the base meaning of words is important.
-  After running this code, the Text_Description column will contain lemmatized text, which should be easier to analyze and more consistent.

# Data Visualization:

## Count of different type of cancers
"""

import matplotlib.pyplot as plt

# Sample data
disease_counts = df['Cancer_Type'].value_counts()

# Define colors for each bar
colors = ['blue', 'green', 'red']

plt.figure(figsize=(10, 6))  # Set a larger figure size
disease_counts.plot(kind='bar', color=colors[:len(disease_counts)])  # Use colors list
plt.title("Types of cancer")
plt.xticks(rotation=45)

# Adding labels to each bar
for i, count in enumerate(disease_counts):
    plt.text(i, count + 0.1, str(count), ha='center', va='bottom')

plt.xlabel('Cancers')  # Label for x-axis
plt.ylabel('Frequency')  # Label for y-axis

plt.tight_layout()
plt.show()

"""- Using matplotlib library, I visualize the distribution of different cancer types in a dataset.
- It provides a clear graphical representation of the frequencies of each cancer type, which can be useful for understanding the dataset's composition and for presentations or reports.
- The bar chart illustrates the distribution of cancer types in the dataset:

  1. Thyroid Cancer: The most prevalent type with a frequency of 2,810 cases.
  2. Colon Cancer: The second most common type with 2,580 cases.
  3. Lung Cancer: The least frequent type among the three, with 2,180 cases.

## Distribution of Cancer Diseases:
"""

plt.figure(figsize=(8, 6))                       # Set a larger figure size for better visibility
counts = df['Cancer_Type'].value_counts()          # Count the occurrences of each cancer type
counts.plot(kind='pie', autopct="%.1f%%")          # Create a pie chart with percentage labels

plt.title("Distribution of Cancer Diseases")       # Set the title of the pie chart

# The `bbox_to_anchor` parameter adjusts the legend's position relative to the figure
plt.legend(counts.index, loc='upper left', bbox_to_anchor=(0.9, 0.9))

plt.show()                                          # Display the pie chart

"""- The pie chart visually represents the distribution of cancer types in the dataset:

  1. Thyroid Cancer: Accounts for 37.1% of the dataset, making it the most prevalent cancer type.
  2. Colon Cancer: Represents 34.1% of the dataset, positioning it as the second most common type.
  3. Lung Cancer: Comprises 28.8% of the dataset, the least frequent among the three types.
- This visualization provides a clear understanding of the relative proportions of each cancer type, highlighting their significance within the dataset.

## Kernel Density Estimation of Word Count in Cancer Type Descriptions:
"""

df.columns

# Calculating the number of words in each text description for different cancer types
num_1 = df[df['Cancer_Type'] == 'Thyroid_Cancer']['Text_Description'].apply(lambda x: len(x.split()))
num_2 = df[df['Cancer_Type'] == 'Colon_Cancer']['Text_Description'].apply(lambda x: len(x.split()))
num_3 = df[df['Cancer_Type'] == 'Lung_Cancer']['Text_Description'].apply(lambda x: len(x.split()))

# Creating a larger figure for better readability
plt.figure(figsize=(12, 8))

# Ploting KDE (Kernel Density Estimate) for the number of words in each cancer type
sns.kdeplot(num_1, shade=True, color='red').set_title('Kernel Density Distribution of Number of Words')
sns.kdeplot(num_2, shade=True, color='blue')
sns.kdeplot(num_3, shade=True, color='green')

# Adding a legend to the plot
plt.legend(labels=['Thyroid_Cancer', 'Colon_Cancer', 'Lung_Cancer'])

# Display the plot
plt.show()

"""- The code calculates the number of words in the text descriptions for each cancer type using lambda x: len(x.split()), which splits the text into words and counts them.

- This KDE plot provides insights into the distribution of text length (in terms of word count) for different cancer types.
- It helps in understanding whether the text descriptions for certain cancer types tend to be longer or shorter on average and how these distributions compare across the cancer types.
- The Kernel Density Estimate (KDE) plot provides a visualization of the distribution of word counts in text descriptions for different cancer types:

  1. Thyroid Cancer (in red):
The density of word counts for Thyroid Cancer is notably above 0.0014.
  2. Colon Cancer (in blue):
The density of word counts for Colon Cancer exceeds that of Thyroid Cancer, reaching above 0.0016. This suggests that the text descriptions for Colon Cancer tend to have a higher average word count compared to Thyroid Cancer.
  3. Lung Cancer (in green):
The density for Lung Cancer is above 0.0006, indicating a lower average word count compared to both Thyroid and Colon Cancer.

## Text Length Distribution in Cancer Descriptions
"""

df['text_len'] = df['Text_Description'].str.len()                 # Compute the length of each text description and store it in a new column

# Create a larger figure for better readability
plt.figure(figsize=(10, 6))

# Plot the histogram of text lengths using seaborn's distplot
# 'color' specifies the color of the histogram
ax = sns.histplot(df['text_len'], color='purple', kde=True)      # Updated to use histplot as distplot is deprecated
ax.set(xlabel='Text Length', ylabel='Frequency')                 # Set labels for the x and y axes
plt.title('Distribution of Text Lengths')                        # Set the title of the histogram

# Display the plot
plt.show()

"""- This histogram visualizes the distribution of text lengths in the dataset, allowing you to understand the variability in the length of text descriptions.
- It shows how frequently different text lengths occur, helping identify any patterns or outliers in the length of the text descriptions across the dataset.

## Distribution of Word Counts in Cancer Descriptions
"""

df['word_count'] = df['Text_Description'].str.split().map(lambda x: len(x))
# Calculate the word count for each text description and store it in a new column

# Create a larger figure for better readability
plt.figure(figsize=(10, 6))

# Ploting the histogram of word counts using seaborn's distplot
# The 'word_count' column represents the number of words in each text description
ax = sns.histplot(df['word_count'], kde=True, color='blue')               # Updated to use histplot as distplot is deprecated
ax.set(xlabel='Word Count', ylabel='Frequency')                           # Set labels for the x and y axes
plt.title('Distribution of Word Counts')                                  # Set the title of the histogram

# Display the plot
plt.show()

"""- This histogram visualizes the distribution of word counts in the text descriptions of your dataset. It helps in understanding the frequency of different word counts and can highlight the typical length of text descriptions.
- This insight can be useful for text processing and analysis, allowing you to identify patterns or anomalies in the length of the text descriptions.

## Visualization of Word Frequency Distribution in Cancer Text Descriptions
"""

from sklearn.feature_extraction.text import CountVectorizer
from yellowbrick.text import FreqDistVisualizer

# Initialize CountVectorizer to convert text data into a matrix of token counts
vectorizer = CountVectorizer()

# Fit the CountVectorizer on the text data and transform the text into a document-term matrix
docs = vectorizer.fit_transform(df['Text_Description'].tolist())

# Get the feature names (i.e., the words) from the vectorizer's vocabulary
features = vectorizer.get_feature_names_out()

# Create a figure for the plot with specified size
fig = plt.figure(figsize=(15, 10))

# Initialize the FreqDistVisualizer to plot the frequency distribution of words
visualizer = FreqDistVisualizer(features=features, orient='h', n=50)


# # Fit the visualizer on the document-term matrix
visualizer.fit(docs)

# Add labels to the x-axis and y-axis
visualizer.ax.set_xlabel('Frequency')  # Label for the x-axis
visualizer.ax.set_ylabel('Words')      # Label for the y-axis
plt.title('Frequency Distribution of Top 50 tokens') # set the title


# Display the frequency distribution visualization
visualizer.show()

"""- This code visualizes the most frequent words in your text data, providing insights into which words are most common in the Text_Description column.
- By focusing on the top 50 words, it allows you to quickly understand the predominant themes or topics in the text data.
"""

# remove words from my dataset
neutral_words = ['the', 'of', 'and', 'in', 'with', 'for', 'by', 'that', 'on', 'from', 'are', 'this', 'be', 'at', 'an', 'we', 'it', 'have', 'which', 'not', 'fig', 'level', 'group', 'using', 'between', 'also', 'used', 'these', 'result', 'all', 'no', 'ha']

df['Text_Description'] = df['Text_Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in neutral_words]))

from sklearn.feature_extraction.text import CountVectorizer
from yellowbrick.text import FreqDistVisualizer

# Initialize CountVectorizer to convert text data into a matrix of token counts
vectorizer = CountVectorizer()

# Fit the CountVectorizer on the text data and transform the text into a document-term matrix
docs = vectorizer.fit_transform(df['Text_Description'].tolist())

# Get the feature names (i.e., the words) from the vectorizer's vocabulary
features = vectorizer.get_feature_names_out()

# Create a figure for the plot with specified size
fig = plt.figure(figsize=(15, 10))

# Initialize the FreqDistVisualizer to plot the frequency distribution of words
visualizer = FreqDistVisualizer(features=features, orient='h', n=20)

# Fit the visualizer on the document-term matrix
visualizer.fit(docs)

# Add labels to the x-axis and y-axis
visualizer.ax.set_xlabel('Frequency')  # Label for the x-axis
visualizer.ax.set_ylabel('Words')      # Label for the y-axis
plt.title('Frequency Distribution of Top 20 tokens') # set the title


# Display the frequency distribution visualization
visualizer.show()

"""- This process is valuable in text analysis as it allows  to filter out unimportant words that might dilute the meaningful content of your dataset.
-  By removing these "neutral" words, the visualization focuses on the more informative tokens, helping to better understand the key themes and concepts in your text data.
-  The final visualization presents the frequency distribution of the top 20 most frequent words remaining after the removal of neutral words, offering a clearer insight into the dataset's content.

## Word Cloud
"""

from wordcloud import WordCloud, STOPWORDS # WordCloud for generating the word cloud, STOPWORDS to exclude common words
import matplotlib.pyplot as plt

df.head()

# Filter the dataframe to include only rows where 'Cancer_Type' is 'Thyroid_Cancer'
_thy_df = df[df['Cancer_Type'] == 'Thyroid_Cancer']

# Filter the dataframe to include only rows where 'Cancer_Type' is 'Colon_Cancer'
_col_df = df[df['Cancer_Type'] == 'Colon_Cancer']

# Filter the dataframe to include only rows where 'Cancer_Type' is 'Lung_Cancer'
_lung_df = df[df['Cancer_Type'] == 'Lung_Cancer']

_thy_df.head()

"""It will show only Thyroid_Cancer."""

# Combine all the text from the 'Text_Description' column of the _thy_df DataFrame into a single string
word_cloud = ' '.join(_thy_df['Text_Description'])

# Created a WordCloud object by generating a word cloud from the 'word_cloud' string
# The 'stopwords' parameter is set to exclude common stopwords from the word cloud
wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(word_cloud)

# Displaying the generated word cloud using matplotlib
plt.imshow(wordcloud)  # The word cloud image generated by the WordCloud object

# Removing the axis to create a cleaner visual representation of the word cloud
plt.axis('off')
plt.show()

"""- I filtered the original dataset to create separate DataFrames for different types of cancer: Thyroid, Colon, and Lung. This allowed us to focus specifically on Thyroid Cancer data.
- Combined all the text descriptions from the 'Text_Description' column of the 'Thyroid_Cancer' DataFrame into a single string.
- This aggregation is essential for generating a meaningful word cloud that reflects the most frequent terms used in the context of 'Thyroid_ Cancer'.
- The generated word cloud was visualized using matplotlib. Here, we can the words which are related to Thyroid_Cancer.
- It will be easier for medical staffs or doctors to identify the symptoms of Thyroid_Cancer.
"""

# Combine all text descriptions from the 'Text_Description' column of the _col_df DataFrame into a single string
word_cloud = ' '.join(_col_df['Text_Description'])

# Generate a word cloud from the combined text for Colon Cancer
# The 'stopwords' parameter is set to exclude common stopwords from the word cloud
wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(word_cloud)

# Display the generated word cloud image using matplotlib
plt.imshow(wordcloud)  # Render the word cloud image
plt.axis('off')  # Remove axis lines and labels for a cleaner visual presentation
plt.show()  # Display the word cloud image

"""- The word cloud visualization represents the most frequent and significant terms from text descriptions related to Colon Cancer. The words displayed in the word cloud are derived from aggregated descriptions and include key symptoms, characteristics, and other relevant terms associated with Colon Cancer.
-While the primary focus is on Colon Cancer, if there are terms related to other cancers (like Breast or Lung Cancer), they may appear in the word cloud if they were mentioned in the descriptions. This could indicate a need for further refinement of the data or highlight discussions about different types of cancer in the context of Colon Cancer.
- Provides a quick and intuitive way to see common symptoms and terms associated with Colon Cancer. This can aid in understanding the prevalence of certain symptoms in patient descriptions and improve diagnostic and treatment strategies.
- Helps in identifying key symptoms that might need more attention in clinical practice or research.
- It is beneficial to healthcare professionals, researchers, patients, and public health advocates, offering valuable insights and aiding in both clinical and educational contexts.
"""

# Combine all text descriptions from the 'Text_Description' column of the _lung_df DataFrame into a single string
word_cloud = ' '.join(_lung_df['Text_Description'])

# Generate a word cloud from the combined text for Lung Cancer
# The 'stopwords' parameter is set to exclude common stopwords from the word cloud
wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(word_cloud)

# Display the generated word cloud image using matplotlib
plt.imshow(wordcloud)  # Render the word cloud image
plt.axis('off')  # Remove axis lines and labels for a cleaner visual presentation
plt.show()  # Display the word cloud image

"""- This visual representation shows the most frequently mentioned symptoms and terms associated with Lung Cancer. Any mentions of other types of cancer (like Breast or Colon Cancer) might appear in the visualization if they were included in the text data.
- It offers a clear visual summary of common symptoms and terms related to Lung Cancer, aiding in diagnostics and treatment by highlighting prevalent symptoms and characteristics.
- Helps in understanding the frequency and prominence of certain symptoms in patient descriptions, which can be valuable for patient care and clinical decision-making.
- This word cloud visualization effectively summarizes and communicates key symptoms and terms related to Lung Cancer from the provided text descriptions.
- It benefits healthcare professionals, researchers, patients, and public health advocates by providing a visual tool for understanding and addressing Lung Cancer symptoms.

# Training and Testing Data:
"""

df['Cancer_Type'].replace({'Colon_Cancer': 0, 'Lung_Cancer': 1, 'Thyroid_Cancer': 2}, inplace=True)

model_data = df.copy()

"""## Model Building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error

vectorizer = TfidfVectorizer(stop_words='english')

X = model_data['Text_Description']
X_vectorized = vectorizer.fit_transform(X)

y = model_data["Cancer_Type"]  # The target variable containing the type of cancer

X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

"""## Required Libraries"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn import preprocessing
import warnings
warnings.filterwarnings('ignore')  # Ignore warnings to keep the output clean
from sklearn.tree import DecisionTreeClassifier

accuracy = {'TF-IDF': []}   # TF-IDF stands for Term Frequency-Inverse Document Frequency

"""- This pipeline prepares the text data for classification, builds models using TF-IDF vectors, and evaluates their performance.
-  By comparing the accuracy and other metrics of different classifiers, we can determine which model best predicts cancer types based on the text descriptions.
-  This approach is fundamental in text classification tasks and helps in understanding the effectiveness of different algorithms on the dataset.

# Machine learning models:

## Logistic Regression
"""

# Initialize the linear model (Logistic Regression in this case)
lr_model = LogisticRegression()

# Train the model on the training data
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)

logistic_accuracy = accuracy_score(y_test, y_pred)
logistic_precision = precision_score(y_test, y_pred, average='macro')
logistic_recall = recall_score(y_test, y_pred, average='macro')
logistic_f1 = f1_score(y_test, y_pred, average='macro')
print(f"Accuracy: {logistic_accuracy}")
print(f"Precision: {logistic_precision}")
print(f"Recall: {logistic_recall}")
print(f"F1 Score: {logistic_f1}")

accuracy['TF-IDF'].append(logistic_accuracy)

"""- The model correctly predicted the cancer type 93.86% of the time on the test set. High accuracy suggests that the Logistic Regression model is a strong performer in this task.
- A precision of 94.36% means that when the model predicts a cancer type, it is correct 94.36% of the time.High precision indicates that the model has a low false positive rate, meaning it rarely classifies a description as a certain cancer type when it is not.
- A recall of 94.4% indicates that the model correctly identifies 94.4% of the actual positive cases. This implies that the model does a good job of capturing most of the positive instances.
- An F1 score of 94.4% indicates that the model has a good balance between precision and recall, performing well in both aspects.
- Overall, logistic regression model seems to be highly effective for the classification task based on these metrics.

## Decision Tree
"""

DT_model = DecisionTreeClassifier(max_depth=2)
DT_model.fit(X_train, y_train)

y_pred = DT_model.predict(X_test)

DT_accuracy = accuracy_score(y_test, y_pred)
DT_precision = precision_score(y_test, y_pred, average='macro')
DT_recall = recall_score(y_test, y_pred,average='macro')
DT_f1 = f1_score(y_test, y_pred,average='macro')
print(f"Accuracy: {DT_accuracy}")
print(f"Precision: {DT_precision}")
print(f"Recall: {DT_recall}")
print(f"F1 Score: {DT_f1}")

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

accuracy['TF-IDF'].append(DT_accuracy)

"""* Class 0: Precision of 63% and recall of 82%, showing better recall but lower precision.
* Class 1: Precision of 72% and recall of 77%, indicating reasonable performance but not as strong as class 0 in recall.
* Class 2: Precision of 94% and recall of 64%, indicating high precision but lower recall, suggesting that this class is identified very accurately but not as comprehensively.
* The decision tree's performance is generally lower than that of the logistic regression model, particularly in terms of overall accuracy and balance between precision and recall.

## Random Forest
"""

RF_model = RandomForestClassifier()
RF_model.fit(X_train, y_train)

RF_model.predict(X_test)
y_pred = RF_model.predict(X_test)

RF_accuracy = accuracy_score(y_test, y_pred)
RF_precision = precision_score(y_test, y_pred, average='macro')
RF_recall = recall_score(y_test, y_pred, average='macro')
RF_f1 = f1_score(y_test, y_pred, average='macro')
print(f"Accuracy: {RF_accuracy}")
print(f"Precision: {RF_precision}")
print(f"Recall: {RF_recall}")
print(f"F1 Score: {RF_f1}")

accuracy['TF-IDF'].append(RF_accuracy)

"""- An accuracy of 100% means that the Random Forest model correctly classified every instance in the test set. This is an ideal outcome, suggesting perfect performance on the test data.
- A precision score of 100% indicates that every positive prediction made by the model was correct. There were no false positives, which is an excellent result.
- A recall score of 100% means that the model identified every actual positive case. There were no false negatives, showing the model's complete effectiveness in capturing all relevant cases.
- An F1 score of 100% confirms that the model has perfect balance between precision and recall. It reflects an ideal performance across both metrics.
- Given the perfect performance, it's important to consider whether the model might be overfitting to the training or test data. Overfitting could occur if the model is too complex relative to the data or if there is a data leakage issue.

## SVM(Support Vector Machine)
"""

import numpy as np
from sklearn.svm import SVC

SVM_model = SVC()
SVM_model.fit(X_train, y_train)

y_pred = SVM_model.predict(X_test)

svm_accuracy = accuracy_score(y_test, y_pred)
svm_precision = precision_score(y_test, y_pred, average='macro')
svm_recall = recall_score(y_test, y_pred, average='macro')
svm_f1 = f1_score(y_test, y_pred, average='macro')
print(f"Accuracy: {svm_accuracy}")
print(f"Precision: {svm_precision}")
print(f"Recall: {svm_recall}")
print(f"F1 Score: {svm_f1}")

accuracy['TF-IDF'].append(svm_accuracy)

"""- An accuracy of 92.1% indicates that the SVM model correctly classified approximately 92% of the instances in the test set. This is a strong performance, though slightly lower than the Random Forest model's perfect accuracy.
- A macro-averaged precision of 92.8% means that, on average, the model's positive predictions are correct 92.8% of the time. This is a high precision, indicating that the model is effective at minimizing false positives.
- A macro-averaged recall of 92.9% indicates that the model is successful in identifying 92.9% of all actual positive cases. This high recall suggests that the SVM is good at capturing positive instances.
- An F1 score of 92.8% reflects a strong balance between precision and recall, showing that the SVM performs well in both areas without significant trade-offs.
- The SVM model performs very well overall, with high accuracy, precision, recall, and F1 score. The metrics are close to those of the Random Forest model, indicating that the SVM is also a strong contender for the classification task.
- Overall, the SVM model demonstrates excellent performance, making it a strong choice for your classification problem alongside the Random Forest model.

## Comparison of all models
"""

# Evaluation
model = ['Logistic','DT', 'RF','SVM']
data = {'model':model,'accuracy':accuracy['TF-IDF']}
compare_model = pd.DataFrame(data)

compare_model

compare_model['accuracy'] = compare_model['accuracy'] *100

compare_model

"""## Plotting accuracies:"""

compare_model["accuracy"].tolist()

import matplotlib.pyplot as plt

# Example data
model = ['Logistic Regression', 'Decision Tree', 'Random Forest','Support Vector Machine']
acc = compare_model["accuracy"].tolist()

# Create the bar plot
plt.figure(figsize=(12, 8))
colors = ['green', 'yellow', 'orange','lightblue']
bars = plt.bar(model, acc, color=colors, edgecolor='black')

# Add data labels on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 3), va='bottom')  # Adjust yval as needed

# Labeling the axes and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('Accuracy', fontsize=14)
plt.title('Comparison of Accuracies of Different Models', fontsize=16)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)

# Adding grid lines for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.tight_layout()
plt.show()

"""- The comparison of model accuracies reveals distinct performance characteristics among the four classifiers. The Random Forest model achieved a perfect accuracy of 100%, indicating exceptional performance in classifying the test data.
- The Logistic Regression model follows with a high accuracy of approximately 93.9%, reflecting strong overall effectiveness.
- The Support Vector Machine (SVM) also performed well, with an accuracy of about 92.1%, showing that it is competitive but slightly less accurate than the Random Forest and Logistic Regression models.
- In contrast, the Decision Tree model lagged behind with an accuracy of approximately 74%, suggesting that its simpler structure (with a maximum depth of 2) may have limited its ability to capture complex patterns in the data.
- Overall, while the Random Forest stands out with perfect accuracy, both the Logistic Regression and SVM models demonstrate strong performance, whereas the Decision Tree may benefit from further tuning or increased complexity

## Naive Bayes Classifier with a Pipeline for Efficient Workflow:
"""

from sklearn.pipeline import Pipeline                        #simplifying machine learning workflows
from sklearn.feature_extraction.text import CountVectorizer  #Converts text data into a matrix of token counts.
from sklearn.naive_bayes import MultinomialNB                #A Naive Bayes classifier suitable for classification with discrete features.

# model building
clf = Pipeline([                          #clf is pipeline variable
    ('vectorizer',CountVectorizer()),     #converts the raw text data into a numerical format
    ('nb',MultinomialNB())                #Naive Bayes classification algorithm to the vectorized text data.
])

from sklearn.model_selection import train_test_split

# Split the data before applying any transformations
X_train, X_test, y_train, y_test = train_test_split(df['Text_Description'], df['Cancer_Type'], test_size=0.2, random_state=42)

# fit the pipeline
clf.fit(X_train, y_train)

# Evaluate the performance of the trained model using the test data
# 'clf.score(X_test, y_test)' computes the accuracy of the model on the test set

accuracy = clf.score(X_test, y_test)

accuracy = clf.score(X_test, y_test)
y_pred = clf.predict(X_test)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""- The Naive Bayes classifier, combined with the CountVectorizer in a pipeline, demonstrates high performance across all metrics, with an accuracy of 93.1%, and very strong precision, recall, and F1 scores.
- This indicates that the model is well-suited for the classification task, performing competitively compared to other models evaluated.
-  The pipeline approach effectively streamlines the process of text vectorization and classification, resulting in a robust and efficient workflow for handling text data.

# Evaluation of Classification Model Performance Using Confusion Matrix and Classification Report:
"""

y_predicted = clf.predict(X_test)

y_predicted

"""- A confusion matrix is a performance measurement for classification problems. It provides a summary of the prediction results on a classification problem."""

from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_predicted)

# Display the confusion matrix
print(cm)

"""- The confusion matrix helps in understanding the performance of the classification model by showing the number of correct and incorrect predictions for each class. It provides insights into how well the model distinguishes between different classes and where it might be making errors.
- Analyzing the confusion matrix is crucial for evaluating model performance, especially in multi-class classification problems. It can help identify specific classes where the model is underperforming and guide further improvements.

-  True Positives (TP): The diagonal elements represent the correctly classified instances for each class.

- True Negatives (TN): The count of instances where the true class is negative (0) and the model also predicts it as negative (0). These are correctly classified negative instances.

-  False Positives (FP): The off-diagonal elements represent the instances that were incorrectly classified.


-  False Negatives (FN): The off-diagonal elements in the respective rows represent the instances that were incorrectly classified as another class.

## Visualizing The Confusion Matrix:
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='RdBu')
plt.xlabel('Predicted Value')
plt.ylabel('True Value')
plt.show()

"""- Based on the heatmap of the confusion matrix:

1. The model generally performs well in correctly identifying instances of classes 1 and 2.
2. There are some misclassifications, particularly between classes 0 and 2.
3. Improving the modelâ€™s ability to distinguish between classes 0 and 2 could potentially enhance overall performance.

## Generating Classification Report:
"""

from sklearn.metrics import classification_report

# Generating the classification report for the test set
# The classification report includes metrics like precision, recall, and F1-score for each class
cl_rep = classification_report(y_test, y_predicted)
print(cl_rep)

"""The model achieves an accuracy of 93%, with macro-averaged scores of 94% for precision, recall, and F1 score, and a weighted average of 93%. This indicates strong and balanced performance across all classes, with particularly outstanding results for Class 1.

# Text Classification Function for Predicting Cancer Types Using a Pre-trained Model:
"""

# Map the string cancer types to integer values in the DataFrame
def predict(text):
    # Use the trained classifier 'clf' to predict the cancer type for the given text description
    prediction = clf.predict([text])

    # Map the predicted integer value back to the corresponding cancer type
    if prediction == 0:
        return 'Colon Cancer'
    elif prediction == 1:
        return 'Lung Cancer'
    elif prediction == 2:
        return 'Thyroid Cancer'

df

# Define the input text for which the cancer type prediction is needed
input_text = "ibrahim almosallam department surgery college medicine qassim university po box buraidah al qassim saudi arabia"

# Call the 'predict' function with the input text to get the predicted cancer type
prediction = predict(input_text)

# Print the predicted cancer type
print(prediction)

"""- This code snippet demonstrates how to use the trained classification model to make predictions based on a new input text. It provides an easy way to see the predicted cancer type for given symptoms or descriptions.
-  This approach can be used in applications for preliminary cancer diagnosis or symptom analysis, helping medical professionals or users classify cancer types based on textual descriptions of symptoms.
- This prediction capability can be used in healthcare settings to assist in the early detection of cancer types based on textual descriptions of symptoms. For example, a medical professional could input patient symptom descriptions into the system to get an initial prediction of the cancer type.
- The model can act as a decision support tool for doctors by providing a preliminary classification that guides further investigation. This can be particularly useful in scenarios where quick initial assessments are needed.
- In summary, the prediction output provides a practical and potentially transformative tool for diagnosing cancer types based on textual symptom descriptions. It supports medical professionals by offering preliminary classifications that can streamline diagnostic workflows, enhance patient interactions, and contribute to ongoing research and education in the medical field.

# Building and Evaluating an LSTM-Based Text Classification Model for Cancer Type Prediction(Neural Network)
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Extract text descriptions and cancer types from the DataFrame
texts = df['Text_Description'].values
cancer_type = df['Cancer_Type'].values

# Tokenize the text
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')  # Initialize tokenizer with a vocabulary size of 10,000
tokenizer.fit_on_texts(texts)  # Build the vocabulary
sequences = tokenizer.texts_to_sequences(texts)  # Convert text to sequences of integers
padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)  # Pad sequences to ensure uniform input length

# Encode the labels
label_encoder = LabelEncoder()  # Initialize the label encoder
labels_encoded = label_encoder.fit_transform(cancer_type)  # Encode the cancer types into integers

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels_encoded, test_size=0.2, random_state=42)

# Create the model
model = Sequential([
    Embedding(input_dim=10000, output_dim=64, input_length=100),  # Embedding layer for converting integers to dense vectors
    LSTM(64, return_sequences=True),  # First LSTM layer with 64 units, returns sequences for the next LSTM layer
    LSTM(64),  # Second LSTM layer with 64 units, returns the last state
    Dense(64, activation='relu'),  # Dense layer with ReLU activation
    Dropout(0.5),  # Dropout layer to prevent overfitting
    Dense(len(np.unique(labels_encoded)), activation='softmax')  # Output layer with softmax activation for multi-class classification
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Use sparse categorical cross-entropy loss for multi-class classification

# Train the model
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)  # Train the model for 10 epochs

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)  # Evaluate the model on the test set
print(f'Loss: {loss}')  # Print the loss
print(f'Accuracy: {accuracy}')  # Print the accuracy

"""- The model achieved a high accuracy of 98.28% on the test set, indicating strong classification performance and effective learning  - The low loss value of 0.0312 suggests that the model's predictions are very close to the true labels, reflecting good fit and minimal errors.
- The model shows rapid improvement over the epochs, with validation accuracy reaching around 98% by Epoch 3 and maintaining high performance through Epoch 10. The loss consistently decreases, indicating effective training.
- The LSTM model, with its embedding, LSTM layers, and dense network, demonstrates excellent performance in classifying text data, achieving high accuracy and low loss. This indicates the model effectively learns from the data and generalizes well to new, unseen examples. The training process was stable, with high validation accuracy indicating robust performance.
"""

import matplotlib.pyplot as plt

# Assuming 'history' is the object returned from model.fit

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

"""1. Accuracy Plot

  - Left Plot: Displays two lines:
    - Training Accuracy: This line represents the accuracy of your model on the training data for each epoch.
    - Validation Accuracy: This line represents the accuracy of your model on the validation data for each epoch.

2. Loss Plot

  - Right Plot: Displays two lines:
    - Training Loss: This line shows the loss (error) on the training data for each epoch.
    - Validation Loss: This line shows the loss on the validation data for each epoch.

# Text Classification Using BERT for Cancer Type Prediction
"""

df.head()

data = df[['Text_Description','Cancer_Type']]

data.head()

data.replace({'Cancer_Type': {'Colon_Cancer': 0, 'Lung_Cancer': 1, 'Thyroid_Cancer': 2}}, inplace=True)

data.head()

import pandas as pd
from datasets import Dataset

# Convert the DataFrame to a Dataset object
dataset = Dataset.from_pandas(data)

# Split the dataset into train and test sets
train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_function(examples):
    return tokenizer(examples['Text_Description'], truncation=True, padding=True)

train_dataset = train_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

"""The dataset has been efficiently prepared for use with BERT, including splitting into training and testing sets and applying tokenization. The text data is now in the appropriate format for BERT-based models, which will allow  to proceed with training and evaluation of your transformer-based model."""

from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
from transformers import Trainer, TrainingArguments, BertForSequenceClassification

def format_dataset(dataset):
    # Ensure labels are in the range [0, 1, 2]
    dataset = dataset.map(lambda examples: {'labels': [0 if label == 0 else (1 if label == 1 else 2) for label in examples['Cancer_Type']]}, batched=True)
    return dataset

train_dataset = format_dataset(train_dataset)
test_dataset = format_dataset(test_dataset)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=3,
    per_device_eval_batch_size=3,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)

from transformers import Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# !pip install accelerate -U

# !pip install accelerate>=0.21.0

import torch
from transformers import Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# torch.autograd.set_detect_anomaly(True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

"""- The training of the BERT model using the Trainer class yielded promising results after one epoch. The model achieved a high validation accuracy of approximately 95.11%, with a corresponding F1 score of 95.114%, indicating balanced performance across all classes.
- Precision and recall are also strong, at 95.125% and 95.112% respectively, reflecting the model's effectiveness in correctly identifying and classifying instances. The training process was efficient, completing with a loss of 0.314, which suggests that the model is learning effectively.
- Overall, the results demonstrate that the model performs well on the classification task, showing robust accuracy and balanced metric scores.
"""